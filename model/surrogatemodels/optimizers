class pso_optimize(Trial):

    #######################
    ## Abstract Methods  ##
    #######################
    
    def initialise(self):
        """
        Initialises the trial and returns True if everything went OK,
        False otherwise.
        """
        self.run_initialize()
        self.state_dictionary['best'] = None
        self.state_dictionary['fitness_evaluated'] = False
        self.state_dictionary['model_failed'] = False
        self.state_dictionary['new_best_over_iteration'] = False
        self.state_dictionary['population'] = None
        self.state_dictionary['best_fitness_array'] = []
        self.state_dictionary['generations_array'] = []
        self.set_main_counter_name("g")
        self.set_counter_dictionary("g",0)
        self.initialize_population()    
        results_folder, images_folder = self.create_results_folder()
        if not results_folder or not images_folder:
            # Results folder could not be created
            logging.error('Results and images folders cound not be created, terminating.')
            return False
        
        return True
        
    def run_initialize(self):
        logging.info("Initialize PSOTrial no:" + str(self.get_trial_no()))
        self.cost_model = DummyCostModel(self.configuration, self.controller, self.fitness)
        design_space = self.fitness.designSpace
        self.toolbox = copy(base.Toolbox())
        self.smin = [-1.0 * self.get_configuration().max_speed *
                (dimSetting['max'] - dimSetting['min'])
                for dimSetting in design_space]
        self.smax = [self.get_configuration().max_speed *
                (dimSetting['max'] - dimSetting['min'])
                for dimSetting in design_space]

        try:
            eval('creator.Particle' + str(self.my_run.get_name()))
            logging.debug("Particle class for this run already exists")
        except AttributeError:
            creator.create('FitnessMax' + str(self.my_run.get_name()), base.Fitness, weights=(1.0,))
            ### we got to add specific names, otherwise the classes are going to be visible for all
            ### modules which use deap...
            
            creator.create(str('Particle' + self.my_run.get_name()), list, fitness=eval('creator.FitnessMax' + str(self.my_run.get_name())),
                           smin=self.smin, smax=self.smax,
                           speed=[uniform(smi, sma) for sma, smi in zip(self.smax,
                                                                        self.smin)],
                           pmin=[dimSetting['max'] for dimSetting in design_space],
                           pmax=[dimSetting['min'] for dimSetting in design_space],
                           model=False, best=None, code=None)

        self.toolbox.register('particle', self.generate, designSpace=design_space)
        self.toolbox.register('filter_particles', self.filterParticles,
                              designSpace=design_space)
        self.toolbox.register('filter_particle', self.filterParticle,
                              designSpace=design_space)
        self.toolbox.register('population', tools.initRepeat,
                              list, self.toolbox.particle)
        self.toolbox.register('update', self.updateParticle, 
                              conf=self.get_configuration(),
                              designSpace=design_space)
        self.toolbox.register('evaluate', self.fitness_function)
        self.new_best=False
        
    def run(self):
        self.state_dictionary['generate'] = True
        
        logging.info(str(self.get_name()) + ' started')
        logging.info('Trial prepared... executing')
        self.save() ## training might take a bit...
        # Initialise termination check
        
        self.check = False
        ## we do this not to retrain model twice during the first iteration. If we ommit
        ## this bit of code the first view_update wont have a model aviable.
        reevalute = False
        if self.state_dictionary["fresh_run"]: ## we need this as we only want to do it for initial generation because the view
            ## the problem is that we cannot
            self.train_surrogate_model()
            self.train_cost_model()
            self.view_update(visualize = True)
            self.state_dictionary["fresh_run"] = False
            self.save()
            
        while self.get_counter_dictionary('g') < self.get_configuration().max_iter + 1:
            
            logging.info('[' + str(self.get_name()) + '] Generation ' + str(self.get_counter_dictionary('g')))
            logging.info('[' + str(self.get_name()) + '] Fitness ' + str(self.get_counter_dictionary('fit')))

            # termination condition - we put it here so that when the trial is reloaded
            # it wont run if the run has terminated already
        # see this
            if self.get_terminating_condition(): 
                logging.info('Terminating condition reached...')
                break
            
            # Roll population
            first_pop = self.get_population().pop(0)
            self.get_population().append(first_pop)
            
            if self.get_counter_dictionary('fit') > self.get_configuration().max_fitness:
                logging.info('Fitness counter exceeded the limit... exiting')
                break
                
            reevalute = False
            # Train surrogate model
            if self.training_set_updated():
                self.train_surrogate_model()
                self.train_cost_model()
                reevalute = True
            #logging.info(str(self.get_population()))
            code, mu, variance, ei, p = self.predict_surrogate_model(self.get_population())
            reloop = False
            if (mu is None) or (variance is None):
                logging.info("Prediction Failed")
                self.set_model_failed(True)
            else:
                logging.info("mean S2 " + str(mean(variance)))
                logging.info("max S2  " + str(max(variance)))
                logging.info("min S2  " + str(min(variance)))
                logging.info("over 0.05  " + str(min(len([v for v in variance if v > 0.05]))))
                logging.info("over 0.01  " + str(min(len([v for v in variance if v > 0.01]))))
                logging.info("mean p " + str(mean(p)))
                logging.info("max p  " + str(max(p)))
                logging.info("min p  " + str(min(p)))
                logging.info("over 0.05  " + str(min(len([v for v in p if v > 0.05]))))
                logging.info("over 0.1  " + str(min(len([v for v in p if v > 0.01]))))
                logging.info("mean ei " + str(mean(ei)))
                logging.info("max ei  " + str(max(ei)))
                logging.info("min ei  " + str(min(ei)))
                reloop = self.post_model_filter(code, mu, variance)
            ##
            if self.get_model_failed():
                logging.info('Model Failed, sampling design space')
                self.sample_design_space() ## we want the local hypercube
            elif reloop:
                reevalute = True
                logging.info('Evaluated some particles, will try to retrain model')
            else:#
                if reevalute:
                    self.reevalute_best()
                # Iteration of meta-heuristic
                self.meta_iterate()
                self.filter_population()
                
                #Check if perturbation is neccesary 
                if self.get_counter_dictionary('g') % self.get_configuration().M == 0:# perturb
                    self.evaluate_best()
                self.new_best = False
            # Wait until the user unpauses the trial.
            while self.get_wait():
                time.sleep(0)
            
            self.increment_main_counter()
            self.view_update(visualize = True)
        self.exit()
        
    ### returns a snapshot of the trial state
    def snapshot(self):
        fitness = self.fitness
        best_fitness_array = copy(self.get_best_fitness_array())
        generations_array = copy(self.get_generations_array())
        results_folder = copy(self.get_results_folder())
        images_folder = copy(self.get_images_folder())
        counter = copy(self.get_counter_dictionary('g'))
        name = self.get_name()
        return_dictionary = {
            'fitness': fitness,
            'best_fitness_array': best_fitness_array,
            'generations_array': generations_array,
            'configuration_folder_path':self.configuration.configuration_folder_path,
            'run_folders_path':self.configuration.results_folder_path,
            'results_folder': results_folder,
            'images_folder': images_folder,
            'counter': counter,
            'counter_dict':  self.state_dictionary['counter_dictionary'] ,
            'timer_dict':  self.state_dictionary['timer_dict'] ,
            'name': name,
            'fitness_state': self.get_fitness_state(),
            'run_name': self.my_run.get_name(),
            'classifier': self.get_classifier(), ## return a copy! 
            'regressor': self.get_regressor(), ## return a copy!
            'cost_model': self.get_cost_model(), ## return a copy!
            'meta_plot': {"particles":{'marker':"o",'data':self.get_population()}},
            'generate' : self.state_dictionary['generate'],
            'max_iter' : self.configuration.max_iter,
            'max_fitness' : self.configuration.max_fitness
        }
        return return_dictionary
        
    def save(self):
        try:
            trial_file = str(self.get_results_folder()) + '/' +  str(self.get_counter_dictionary('g')) + '.txt'
            dict = self.state_dictionary
            surrogate_model_state_dict = self.surrogate_model.get_state_dictionary()
            cost_model_state_dict = self.cost_model.get_state_dictionary()
            dict['cost_model_state_dict'] = cost_model_state_dict
            dict['surrogate_model_state_dict'] = surrogate_model_state_dict
            with io.open(trial_file, 'wb') as outfile:
                pickle.dump(dict, outfile)  
                if self.kill:
                    sys.exit(0)
        except Exception, e:
            logging.error(str(e))
            if self.kill:
                sys.exit(0)
            return False
            
    ## by default find the latest generation
    def load(self, generation = None):
        try:
            if generation is None:
                # Figure out what the last generation before crash was
                found = False
                for filename in reversed(os.listdir(self.get_results_folder())):
                    match = re.search(r'^(\d+)\.txt', filename)
                    if match:
                        # Found the last generation
                        generation = int(match.group(1))
                        found = True
                        break

                if not found:
                    return False
                    
            generation_file = str(generation)
            trial_file = str(self.get_results_folder()) + '/' + str(generation_file) + '.txt'
            
            with open(trial_file, 'rb') as outfile:
                dict = pickle.load(outfile)
            self.set_state_dictionary(dict)
            self.state_dictionary["generate"] = False
            self.kill = False
            self.surrogate_model.set_state_dictionary(dict['surrogate_model_state_dict'])
            self.cost_model.set_state_dictionary(dict['cost_model_state_dict'])
            self.previous_time = datetime.now()
            logging.info("Loaded Trial")
            return True
        except Exception, e:
            logging.error("Loading error" + str(e))
            return False
        
    ####################
    ## Helper Methods ##
    ####################
        
    def checkCollapse(self): #TODO
        ## this method checks if the particls
        ## a) collapsed onto a single point
        ## b) collapsed onto the edge of the search space
        ## if so it reintializes them.
        minimum_diverity = 0.95 ##if over 95 collapsed reseed
        
        
        if collapsed:
            self.set_population(self.toolbox.population(self.get_configuration().population_size))
            self.toolbox.filter_particles(self.get_population())
    
    def create_particle(self, particle):
        return eval('creator.Particle' + self.my_run.get_name())(particle)
        
    def createUniformSpace(self, particles, designSpace):
        pointPerDimensions = 5
        valueGrid = mgrid[designSpace[0]['min']:designSpace[0]['max']:
                          complex(0, pointPerDimensions),
                          designSpace[1]['min']:designSpace[1]['max']:
                          complex(0, pointPerDimensions)]

        for i in [0, 1]:
            for j, part in enumerate(particles):
                part[i] = valueGrid[i].reshape(1, -1)[0][j]

    def filterParticles(self,  particles, designSpace):
        for particle in particles:
            self.filterParticle(particle, designSpace)
            
    def filterParticle(self, p, designSpace):
        p.pmin = [dimSetting['min'] for dimSetting in designSpace]
        p.pmax = [dimSetting['max'] for dimSetting in designSpace]

        for i, val in enumerate(p):
            #dithering
            if designSpace[i]['type'] == 'discrete':
                if uniform(0.0, 1.0) < (p[i] - floor(p[i])):
                    p[i] = ceil(p[i])  # + designSpace[i]['step']
                else:
                    p[i] = floor(p[i])

            #dont allow particles to take the same value
            p[i] = minimum(p.pmax[i], p[i])
            p[i] = maximum(p.pmin[i], p[i])

    def generate(self,  designSpace):
        particle = [uniform(dimSetting['min'], dimSetting['max'])
                    for dimSetting
                    in designSpace]
        particle = self.create_particle(particle)
        return particle

    # update the position of the particles
    # should change this part in order to change the leader election strategy
    def updateParticle(self,  part, generation, conf, designSpace):
        if conf.admode == 'fitness':
            fraction = self.fitness_counter / conf.max_fitness
        elif conf.admode == 'iter':
            fraction = generation / conf.max_iter
        else:
            raise('[updateParticle]: adjustment mode unknown.. ')

        u1 = [uniform(0, conf.phi1) for _ in range(len(part))]
        u2 = [uniform(0, conf.phi2) for _ in range(len(part))]
        
        ##########   this part particulately, leader election for every particle
        v_u1 = map(operator.mul, u1, map(operator.sub, part.best, part))
        v_u2 = map(operator.mul, u2, map(operator.sub, self.get_best(), part))
        weight = 1.0
        if conf.weight_mode == 'linear':
            weight = conf.max_weight - (conf.max_weight -
                                        conf.min_weight) * fraction
        elif conf.weight_mode == 'norm':
            weight = conf.weight
        else:
            raise('[updateParticle]: weight mode unknown.. ')
        weightVector = [weight] * len(part.speed)
        part.speed = map(operator.add,
                         map(operator.mul, part.speed, weightVector),
                         map(operator.add, v_u1, v_u2))

    # what's this mean?
        if conf.applyK is True:
            phi = array(u1) + array(u1)

            XVector = (2.0 * conf.KK) / abs(2.0 - phi -
                                            sqrt(pow(phi, 2.0) - 4.0 * phi))
            part.speed = map(operator.mul, part.speed, XVector)

    # what's the difference between these modes?
        if conf.mode == 'vp':
            for i, speed in enumerate(part.speed):
                speedCoeff = (conf.K - pow(fraction, conf.p)) * part.smax[i]
                if speed < -speedCoeff:
                    part.speed[i] = -speedCoeff
                elif speed > speedCoeff:
                    part.speed[i] = speedCoeff
                else:
                    part.speed[i] = speed
        elif conf.mode == 'norm':
            for i, speed in enumerate(part.speed):
                if speed < part.smin[i]:
                    part.speed[i] = part.smin[i]
                elif speed > part.smax[i]:
                    part.speed[i] = part.smax[i]
        elif conf.mode == 'exp':
            for i, speed in enumerate(part.speed):
                maxVel = (1 - pow(fraction, conf.exp)) * part.smax[i]
                if speed < -maxVel:
                    part.speed[i] = -maxVel
                elif speed > maxVel:
                    part.speed[i] = maxVel
        elif conf.mode == 'no':
            pass
        else:
            raise('[updateParticle]: mode unknown.. ')
        part[:] = map(operator.add, part, part.speed)

    def initialize_population(self):
        ## the while loop exists, as meta-heuristic makes no sense till we find at least one particle that is within valid region...
        
        try:
            part = self.create_particle(self.get_configuration().always_valid)
            self.toolbox.filter_particle(part)
            part.fitness.values, part.code, cost = self.fitness_function(part)
            if not self.get_best() or self.is_better(part.fitness, self.get_best().fitness):
                particle = self.create_particle(part)
                particle.fitness.values = part.fitness.values
                self.set_best(particle)
            self.set_at_least_one_in_valid_region(True)
            logging.info("Always valid configuration present, evaluated")
        except:
            logging.info("Always valid configuration not-present, make sure that the valid design space is large enough so that at least one valid design is initially evalauted")
            self.set_at_least_one_in_valid_region(False)
        self.set_at_least_one_in_valid_region(True)
        logging.info(str(self.get_at_least_one_in_valid_region()))
        #F = copy(self.get_configuration().F)
        designSpace = self.fitness.designSpace
        D = len(designSpace)
        latin_hypercube_samples = lhs.lhs(scipy_uniform,[0,1],(self.get_configuration().population_size,D))
        max_bounds = array([d["max"] for d in designSpace])
        min_bounds = array([d["min"] for d in designSpace])
        latin_hypercube_samples = latin_hypercube_samples * (max_bounds - min_bounds)
        latin_hypercube_samples = latin_hypercube_samples + min_bounds
        population = []
        f_counter = 0
        for part in latin_hypercube_samples:
            part = self.create_particle(part)
            self.toolbox.filter_particle(part)
            if f_counter < self.get_configuration().population_size/1.:
                part.fitness.values, part.code, cost = self.fitness_function(part)
                self.set_at_least_one_in_valid_region((part.code == 0) or self.get_at_least_one_in_valid_region())
                if not self.get_best() or self.is_better(part.fitness, self.get_best().fitness):
                    particle = self.create_particle(part)
                    particle.fitness.values = part.fitness.values
                    self.set_best(particle)
            population.append(part)
            f_counter = f_counter + 1
        self.set_population(population)
        while (not self.get_at_least_one_in_valid_region()):
            #logging.in
            exit(0)
            part = self.toolbox.particle() ## a random particle
            logging.info("All particles within invalid search space.. Evaluating extra examples: " + str(part))
            part.fitness.values, part.code, cost = self.toolbox.evaluate(part)
            self.set_at_least_one_in_valid_region((part.code == 0) or self.get_at_least_one_in_valid_region())
            if not self.get_best() or self.is_better(part.fitness, self.get_best().fitness):
                particle = self.create_particle(part)
                particle.fitness.values = part.fitness.values
                self.set_best(particle)
                        
        self.state_dictionary["fresh_run"] = True
        
        #what's this function do?
    def meta_iterate(self):
        #TODO - reavluate one random particle... do it.. very important!
        ##while(self.get_at_least_one_in_valid_region()):
        ##    logging.info("All particles within invalid area... have to randomly sample the design space to find one that is OK...")             
            
        #Update Bests
        logging.info("Meta Iteration")
        for part in self.get_population():
            if not part.best or self.is_better(part.fitness, part.best.fitness):
                part.best = self.create_particle(part)
                part.best.fitness.values = part.fitness.values
            if not self.get_best() or self.is_better(part.fitness, self.get_best().fitness):
                particle = self.create_particle(part)
                particle.fitness.values = part.fitness.values
                self.set_best(particle)
                self.new_best = True
                                
        #PSO
        for part in self.get_population():
            self.toolbox.update(part, self.get_counter_dictionary('g'))

    def filter_population(self):
        self.toolbox.filter_particles(self.get_population())
   
    def evaluate_best(self):        
        if self.new_best:
            self.fitness_function(self.get_best())
            logging.info('New best was found after M :' + str(self.get_best()))
        else:            
            ## TODO - clean it up...  messy
            perturbation = self.perturbation(radius = 100.0)                        
            logging.info('Best was already evalauted.. adding perturbation ' + str(perturbation))
            perturbed_particle = self.create_particle(self.get_best())
            code, mean, variance, ei, p = self.predict_surrogate_model([perturbed_particle])
            if code is None:
                logging.debug("Code is none..watch out")
            if code[0] == 0:
                logging.info('Perturbation might be valid, evaluationg')
            for i,val in enumerate(perturbation):
                perturbed_particle[i] = perturbed_particle[i] + val       
            self.toolbox.filter_particle(perturbed_particle)
            if self.surrogate_model.contains_training_instance(perturbed_particle):
                logging.info('Perturbation was already evaluated.. sampling')
                self.sample_design_space()
            else:
                fitness, code, cost = self.fitness_function(perturbed_particle) 
                ##check if the value is not a new best
                perturbed_particle.fitness.values = fitness
                if not self.get_best() or self.is_better(perturbed_particle.fitness, self.get_best().fitness):
                    self.set_best(perturbed_particle)
            #else: ## why do we do this? because regressor needs to be updated
            #    if code[0] != 0:
            #        logging.info('Best is within the invalid area ' + str(code[0]) + ', sampling design space')
            #        self.sample_design_space()
            
        
    def increment_main_counter(self):
        self.get_best_fitness_array().append(self.get_best().fitness.values[0])
        self.get_generations_array().append(self.get_counter_dictionary(self.get_main_counter_name()))
        self.save()
        self.increment_counter(self.get_main_counter_name())

    def sample_design_space(self):
        #logging.info('Evaluating best perturbation')
        particle = self.surrogate_model.max_ei(designSpace=self.fitness.designSpace, hypercube = self.hypercube())
        if particle is None:
            logging.info("Local sampling has failed, probably all of the particles are within invalid region")
            particle = self.surrogate_model.max_ei(designSpace=self.fitness.designSpace)
            if particle is None:
                perturbation = self.perturbation(radius = 100.0)                        
                logging.info('Evaluating random perturbation of real best ' + str(perturbation))
                particle = self.create_particle(self.surrogate_model.get_best()[0])
                for i,val in enumerate(perturbation):
                    particle[i] = particle[i] + val       
                logging.info("Global sampling failed as well.. Evaluating a random particle"  + str(particle))
        particle = self.create_particle(particle)   
        self.toolbox.filter_particle(particle)
        particle.fitness.values, code, cost = self.fitness_function(particle) 
        if not self.get_best() or self.is_better(particle.fitness, self.get_best().fitness):
            self.set_best(particle)
            
     ## not used currently
    def get_dist(self):
        if best:
            distances = sqrt(sum(pow((self.surrogate.best),2),axis=1))  # TODO
            order_according_to_manhatan = argsort(distances)
            closest_array = [gpTrainingSet[index] for index in order_according_to_manhatan[0:conf.nClosest]]
        ###        
        ## limit to hypercube around the points
        #find maximum
        #print "[getDist] closestArray ",closestArray
        max_diag = deepcopy(closestArray[0])
        for part in closest_array:
            max_diag = maximum(part, max_diag)
        ###find minimum vectors
        min_diag = deepcopy(closest_array[0])
        for part in closest_array:
            min_diag = minimum(part, min_diag)
        return [max_diag, min_diag]
        
    ### a hypercube that contains all the particles
    def hypercube(self):
        #find maximum
        max_diag = deepcopy(self.get_population()[0])
        for part in self.get_population():
            max_diag = maximum(part,max_diag) 
        ###find minimum vectors
        min_diag = deepcopy(self.get_population()[0])
        for part in self.get_population():
            min_diag = minimum(part,min_diag)
            
        ## we always ensure that the hypercube allows particles to maintain velocity components in all directions
        
        for i,dd in enumerate(max_diag):
            if self.fitness.designSpace[i]["type"] == "discrete":
                max_diag[i] = minimum(dd + self.fitness.designSpace[i]["step"],self.fitness.designSpace[i]["max"])
            elif self.fitness.designSpace[i]["type"] == "continuous":
                small_fraction = ((self.fitness.designSpace[i]["max"] - self.fitness.designSpace[i]["min"]) / 100.)
                max_diag[i] = minimum(dd + small_fraction, self.fitness.designSpace[i]["max"])
                
        for i,dd in enumerate(min_diag):
            if self.fitness.designSpace[i]["type"] == "discrete":
                min_diag[i] = maximum(dd - self.fitness.designSpace[i]["step"],self.fitness.designSpace[i]["min"])
            elif self.fitness.designSpace[i]["type"] == "continuous":
                small_fraction = ((self.fitness.designSpace[i]["max"] - self.fitness.designSpace[i]["min"]) / 100.)
                min_diag[i] = maximum(dd - small_fraction, self.fitness.designSpace[i]["min"])
        logging.info("hypecube: " + str([max_diag,min_diag]))
        return [max_diag,min_diag]
        
    def perturbation(self, radius = 10.0):
        [max_diag,min_diag] = self.hypercube()
        d = (max_diag - min_diag)/radius
        for i,dd in enumerate(d):
            if self.fitness.designSpace[i]["type"] == "discrete":
                d[i] = maximum(dd,self.fitness.designSpace[i]["step"])
            elif self.fitness.designSpace[i]["type"] == "continuous":
                small_fraction = ((self.fitness.designSpace[i]["max"] - self.fitness.designSpace[i]["min"]) / 100.)
                d[i] = maximum(dd,small_fraction)
        dimensions = len(self.fitness.designSpace)
        pertubation =  multiply(((rand(1,dimensions)-0.5)*2.0),d)[0] #TODO add the dimensions
        return pertubation
    
    ### TODO - its just copy and pasted ciode now..w could rewrite it realyl
    def post_model_filter(self, code, mean, variance):
        eval_counter = 1
        self.set_model_failed(not (False in [self.get_configuration().max_stdv < pred for pred in variance]))
        if self.get_model_failed():
            return False
        if (code is None) or (mean is None) or (variance is None):
            self.set_model_failed(False)
        else:
            #### if all particles that have stdv > max been evalauted we have a prbolem and we shoudl do something...
            #### currently we sample design space again
            #### this can happen during first iteration...
            #all_evaled = True
            #counter = 0
            #for (p, c, m, v) in zip(self.get_population(), code, mean, variance):
            #    if ((v > self.get_configuration().max_stdv) and (c == 0)):
            #        all_evaled = all_evaled and (self.get_surrogate_model().contains_training_instance(p))
            #        counter = counter + 1
                    
            #if all_evaled and counter: ## randomize population
            #    logging.info("Houston... we got a problem.. this might happen at the beggining" + str(all_evaled) + " "  + str(counter) + " " + str(zip(self.get_population(), code, mean, variance)))
            #    self.sample_design_space()
            #    return True
                
            for i, (p, c, m, v) in enumerate(zip(self.get_population(), code, mean, variance)):
                if v > self.get_configuration().max_stdv and c == 0:
                    if eval_counter > self.get_configuration().max_eval:
                        logging.info("Evalauted more fitness functions per generation then max_eval")
                        return True
                    p.fitness.values, p.code, cost = self.toolbox.evaluate(p)
                    eval_counter = eval_counter + 1
                else:
                    try:
                        if c == 0:
                            p.fitness.values = m
                        else:
                            p.fitness.values = [self.fitness.worst_value]
                    except:
                        p.fitness.values, p.code, cost = self.toolbox.evaluate(p)
            ## at least one particle has to have std smaller then max_stdv
            ## if all particles are in invalid zone
        return False
   
    def reevalute_best(self):
        bests_to_model = [p.best for p in self.get_population() if p.best] ### Elimate Nones -- in case M < Number of particles, important for initialb iteratiions
        if self.get_best():
            bests_to_model.append(self.get_best())
        if bests_to_model:
            logging.info("Reevaluating")
            code, bests_to_fitness, variance, ei, p = self.predict_surrogate_model(bests_to_model)
            if (code is None) or (bests_to_fitness is None) or (variance is None):
                logging.info("Prediction failed during reevaluation... omitting")
            else:
                for i,part in enumerate([p for p in self.get_population() if p.best]):
                    if code[i] == 0:
                        part.best.fitness.values = bests_to_fitness[i]
                    else:
                        part.best.fitness.values = [self.fitness.worst_value]
                if self.get_best():
                    best = self.get_best()
                    if code[-1] == 0:
                        best.fitness.values = bests_to_fitness[-1]
                    else:
                        best.fitness.values = [self.fitness.worst_value]
                    ## find best among the training set!!!
                    logging.info("Fixing best: " + str(best))
                    evaled_best, evaled_best_fitness = self.surrogate_model.get_best()
                    logging.info(str(evaled_best))
                    evaled_best = self.create_particle(evaled_best)   
                    self.toolbox.filter_particle(evaled_best)
                    evaled_best.fitness.values = evaled_best_fitness
                    if self.is_better(evaled_best_fitness, best.fitness.values):
                        logging.info("Real best better: " + str(evaled_best))
                        self.set_best(evaled_best)
                    
    #######################
    ### GET/SET METHODS ###
    #######################
    
    def get_predicted_time(self):
        predicted_time = self.state_dictionary['total_time'] * self.get_configuration().max_iter / (self.get_counter_dictionary('g') + 1.0)
        return str(timedelta(seconds=predicted_time))
    
    def set_population(self, population):
        self.state_dictionary["population"] = population
        
    def get_population(self):
        return self.state_dictionary["population"]

    def get_cost_model(self): ## returns a copy of the model... quite important not to return the model itself as ll might get F up
        model = DummyCostModel(self.get_configuration(), self.controller, self.fitness)
        model.set_state_dictionary(self.cost_model.get_state_dictionary())
        return model
 